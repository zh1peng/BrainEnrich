---
title: "Precompute null gene set scores on HPC with slurm"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Precompute null gene set scores on HPC with slurm}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Installation of the BrainEnrich package
```{r setup, eval=TRUE}
# Install remotes if you haven't already
if (!requireNamespace("remotes", quietly = TRUE)) {
  install.packages("remotes")
}
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}
BiocManager::install("DOSE")
# Install brainEnrich from GitHub
remotes::install_github("zh1peng/BrainEnrich")
```

# 2. Download gene_data and annoData to the package
We only need to do this once, and they will be available for all the nodes in the HPC.
If you download them during the slurm job, it might cause the job to fail due to the sequential download of the data.
The first node is trying to download the data, and the other nodes are trying to access the data that is not yet downloaded.
```{r}
library(BrainEnrich)
gene_data <- get_geneExp(atlas = "desikan", rdonor = "r0.4", hem = "L")
annoData <- get_annoData(type = "SynGO")
```

# 2. Prepare slurm script
A script for distributed computing environments, allowing large-scale permutation tests to be split across multiple compute nodes. 
```{r}
# run_brainscore_hpc.R
# Rscript run_brainscore_hpc.R <job_id> <n_perm_per_job> <perm_total> <cor_method> <aggre_method> <null_model> <minGSSize> <maxGSSize>
# Get command-line arguments
args <- commandArgs(trailingOnly = TRUE)
job_id <- as.integer(args[1])
n_perm_per_job <- as.integer(args[2])
perm_total <- as.integer(args[3])
cor_method <- args[4]
aggre_method <- args[5]
null_model <- args[6]
minGSSize <- as.integer(args[7])
maxGSSize <- as.integer(args[8])

# Load your R package and any required libraries
library(BrainEnrich)

# Set other parameters
output_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed"
data_path <- "/gpfs1/home/z/c/zcao4/BrainEnrich/data"

# Load hcp brain_data
brain_data <- readRDS(file.path(data_path, "hcp_brain_data_dk_lh.RDS"))
# Load additional necessary data
gene_data <- get_geneExp(atlas = "desikan", rdonor = "r0.4", hem = "L")
annoData <- get_annoData(type = "SynGO")

if (null_model == "spin_brain") {
  perm_id <- perm_id_dk_lh_5000
} else {
  perm_id <- NULL
}

# Update the output directory based on the input parameters
output_dir <- file.path(
  output_dir_base,
  sprintf(
    "null_%s_%s_%s_%d_%d_%d",
    null_model,
    cor_method,
    aggre_method,
    minGSSize,
    maxGSSize,
    perm_total
  )
)


# Call the brainscore.hpc function
brainscore.hpc(
  job_id = job_id,
  n_perm_per_job = n_perm_per_job,
  perm_total = perm_total,
  perm_id = perm_id,
  output_dir = output_dir,
  brain_data = brain_data,
  gene_data = gene_data,
  annoData = annoData,
  cor_method = cor_method,
  aggre_method = aggre_method,
  null_model = null_model,
  n_cores = 1,
  minGSSize = minGSSize,
  maxGSSize = maxGSSize
)
```

# 3. Prepare slurm script
```{slurm}
#!/bin/bash
#SBATCH --job-name=gsScore       # Job name
#SBATCH --output=gsScore.out  
#SBATCH --error=gsScore.err   
#SBATCH --time=24:00:00                 # Time limit hrs:min:sec
#SBATCH --nodes=1                       # Number of nodes
#SBATCH --ntasks=1                      # Number of tasks
#SBATCH --mem=16G                       # Memory per node
#SBATCH --array=1-5                     # Array range (adjust based on perm_total / n_perm_per_job)

# Define variables for the script
n_perm_per_job=1          # Number of permutations per job (adjust as needed)
perm_total=5000           # Total number of permutations
cor_method="pearson"      # Correlation method
aggre_method="mean"       # Aggregation method
null_model="spin_brain"   # Null model
minGSSize=20              # Minimum gene set size
maxGSSize=200             # Maximum gene set size

# Run the R script with the appropriate arguments
cd /gpfs1/home/z/c/zcao4/BrainEnrich
Rscript --vanilla run_brainscore_hpc.R $SLURM_ARRAY_TASK_ID $n_perm_per_job $perm_total $cor_method $aggre_method $null_model $minGSSize $maxGSSize
```

# 4. Submit the job
```{bash}
sbatch run_brainscore_hpc.slurm
```

# 5. collect the results
```{r}
library(BrainEnrich)
output_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed"

n_jobs <- 5 # Total number of permutations
cor_method <- "pearson" # Correlation method
aggre_method <- "mean" # Aggregation method
null_model <- "spin_brain" # Null model
minGSSize <- 20 # Minimum gene set size
maxGSSize <- 200 # Maximum gene set size

output_dir <- file.path(
  output_dir_base,
  sprintf(
    "null_%s_%s_%s_%d_%d_%d",
    null_model,
    cor_method,
    aggre_method,
    minGSSize,
    maxGSSize,
    perm_total
  )
)
save_name <- sprintf("combined_%s_%s_%s_%d_%d_%d", null_model, cor_method, aggre_method, minGSSize, maxGSSize, perm_total)
gsScoreList.null <- combine.rds(output_dir = output_dir, n_rds = n_jobs, save_name = save_name, delete_originals = TRUE)
```

# 6. use job_splitter (a generic function to split jobs) to do brainscore and brainscore.simulate
A script for distributed computing environments, allowing large-scale permutation tests to be split across multiple compute nodes. 
```{r}
# run_brainscore_hpc.R
# Rscript run_brainscore_hpc.R <job_id> <n_perm_per_job> <perm_total> <cor_method> <aggre_method> <null_model> <minGSSize> <maxGSSize>
# Get command-line arguments
args <- commandArgs(trailingOnly = TRUE)
job_id <- as.integer(args[1])
n_iter_per_job <- as.integer(args[2])
iter_total <- as.integer(args[3])
cor_method <- args[4]
aggre_method <- args[5]
null_model <- args[6]
minGSSize <- as.integer(args[7])
maxGSSize <- as.integer(args[8])

# Load your R package and any required libraries
library(BrainEnrich)

# Set other parameters
output_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed"
data_path <- "/gpfs1/home/z/c/zcao4/BrainEnrich/data"

# Load hcp brain_data
brain_data <- readRDS(file.path(data_path, "hcp_brain_data_dk_lh.RDS"))
# Load additional necessary data
gene_data <- get_geneExp(atlas = "desikan", rdonor = "r0.4", hem = "L")
annoData <- get_annoData(type = "SynGO")

if (null_model == "spin_brain") {
  perm_id <- perm_id_dk_lh_5000
} else {
  perm_id <- NULL
}

# Update the output directory based on the input parameters
output_dir <- file.path(
  output_dir_base,
  sprintf(
    "null_%s_%s_%s_%d_%d_%d",
    null_model,
    cor_method,
    aggre_method,
    minGSSize,
    maxGSSize,
    perm_total
  )
)


# Call the brainscore.hpc function
job_splitter(
  job_id = job_id,
  n_iter_per_job = n_iter_per_job,
  iter_total = iter_total,
  save_name = 'res_job_'
  output_dir = output_dir,
  brain_data = brain_data,
  gene_data = gene_data,
  annoData = annoData,
  cor_method = cor_method,
  aggre_method = aggre_method,
  null_model = null_model,
  n_cores = 1,
  minGSSize = minGSSize,
  maxGSSize = maxGSSize
)
```


