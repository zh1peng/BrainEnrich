---
title: "Precompute null gene set scores on HPC with slurm"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Precompute null gene set scores on HPC with slurm}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Installation of the BrainEnrich package
```{r setup, eval=TRUE}
# Install remotes if you haven't already
if (!requireNamespace("remotes", quietly = TRUE)) {
  install.packages("remotes")
}
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}
BiocManager::install("DOSE")
# Install brainEnrich from GitHub
remotes::install_github("zh1peng/BrainEnrich")
```

# 2. Download gene_data and annoData to the package
We only need to do this once, and they will be available for all the nodes in the HPC.
If you download them during the slurm job, it might cause the job to fail due to the sequential download of the data.
The first node is trying to download the data, and the other nodes are trying to access the data that is not yet downloaded.
```{r}
library(BrainEnrich)
gene_data <- get_geneExp(atlas = "desikan", rdonor = "r0.4", hem = "L")
annoData <- get_annoData(type = "SynGO")
annoData <- get_annoData(type = "GO_MF")
```

# 2. Prepare slurm script
A script for distributed computing environments, allowing large-scale permutation tests to be split across multiple compute nodes. 
```{r}
# run_brainscore_hpc.R
# Rscript run_brainscore_hpc.R <job_id> <n_perm_per_job> <perm_total> <cor_method> <aggre_method> <null_model> <minGSSize> <maxGSSize>
# Get command-line arguments
args <- commandArgs(trailingOnly = TRUE)
job_id <- as.integer(args[1])
n_perm_per_job <- as.integer(args[2])
perm_total <- as.integer(args[3])
cor_method <- args[4]
aggre_method <- args[5]
null_model <- args[6]
minGSSize <- as.integer(args[7])
maxGSSize <- as.integer(args[8])

# Load your R package and any required libraries
library(BrainEnrich)

# Set other parameters
output_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed"
data_path <- "/gpfs1/home/z/c/zcao4/BrainEnrich/data"

# Load hcp brain_data
brain_data <- readRDS(file.path(data_path, "hcp_brain_data_dk_lh.RDS"))
# Load additional necessary data
gene_data <- get_geneExp(atlas = "desikan", rdonor = "r0.4", hem = "L")
annoData <- get_annoData(type = "SynGO")

if (null_model == "spin_brain") {
  perm_id <- perm_id_dk_lh_5000
} else {
  perm_id <- NULL
}

# Update the output directory based on the input parameters
output_dir <- file.path(
  output_dir_base,
  sprintf(
    "null_%s_%s_%s_%d_%d_%d",
    null_model,
    cor_method,
    aggre_method,
    minGSSize,
    maxGSSize,
    perm_total
  )
)


# Call the brainscore.hpc function
brainscore.hpc(
  job_id = job_id,
  n_perm_per_job = n_perm_per_job,
  perm_total = perm_total,
  perm_id = perm_id,
  output_dir = output_dir,
  brain_data = brain_data,
  gene_data = gene_data,
  annoData = annoData,
  cor_method = cor_method,
  aggre_method = aggre_method,
  null_model = null_model,
  n_cores = 1,
  minGSSize = minGSSize,
  maxGSSize = maxGSSize
)
```

# 3. Prepare slurm script
```{bash}
#!/bin/bash
#SBATCH --job-name=gsScore       # Job name
#SBATCH --output=gsScore.log    
#SBATCH --time=24:00:00                 # Time limit hrs:min:sec
#SBATCH --nodes=1                       # Number of nodes
#SBATCH --ntasks=1                      # Number of tasks
#SBATCH --mem=16G                       # Memory per node
#SBATCH --array=1-1000                     # Array range (adjust based on perm_total / n_perm_per_job)

# Define variables for the script
n_perm_per_job=5          # Number of permutations per job (adjust as needed)
perm_total=5000          # Total number of permutations
cor_method="pearson"      # Correlation method
aggre_method="mean"       # Aggregation method
null_model="resample_gene"   # Null model
minGSSize=20              # Minimum gene set size
maxGSSize=200             # Maximum gene set size

# Run the R script with the appropriate arguments
cd /gpfs1/home/z/c/zcao4/BrainEnrich
Rscript --vanilla run_brainscore_hpc.R $SLURM_ARRAY_TASK_ID $n_perm_per_job $perm_total $cor_method $aggre_method $null_model $minGSSize $maxGSSize

null_model="spin_brain"   # Null model
Rscript --vanilla run_brainscore_hpc.R $SLURM_ARRAY_TASK_ID $n_perm_per_job $perm_total $cor_method $aggre_method $null_model $minGSSize $maxGSSize
```

# 4. Submit the job
```{bash}
sbatch run_brainscore_hpc.slurm
```

# 5. collect the results
```{r}
library(BrainEnrich)
output_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed"

n_jobs <- 5 # Total number of permutations
cor_method <- "pearson" # Correlation method
aggre_method <- "mean" # Aggregation method
null_model <- "spin_brain" # Null model
minGSSize <- 20 # Minimum gene set size
maxGSSize <- 200 # Maximum gene set size

output_dir <- file.path(
  output_dir_base,
  sprintf(
    "null_%s_%s_%s_%d_%d_%d",
    null_model,
    cor_method,
    aggre_method,
    minGSSize,
    maxGSSize,
    perm_total
  )
)
save_name <- sprintf("combined_%s_%s_%s_%d_%d_%d", null_model, cor_method, aggre_method, minGSSize, maxGSSize, perm_total)
gsScoreList.null <- combine.rds(output_dir = output_dir, n_rds = n_jobs, save_name = save_name, delete_originals = TRUE)
```

# 6. use job_splitter (a generic function to split jobs) to do brainscore
A script for distributed computing environments, allowing large-scale permutation tests to be split across multiple compute nodes. 
```{r}
# run_brainscore_hpc.R
# Rscript run_brainscore_hpc.R <job_id> <n_perm_per_job> <perm_total> <cor_method> <aggre_method> <null_model> <minGSSize> <maxGSSize>
# Get command-line arguments
args <- commandArgs(trailingOnly = TRUE)
job_id <- as.integer(args[1])
n_perm_per_job <- as.integer(args[2])
perm_total <- as.integer(args[3])
cor_method <- args[4]
aggre_method <- args[5]
null_model <- args[6]
minGSSize <- as.integer(args[7])
maxGSSize <- as.integer(args[8])

# Load your R package and any required libraries
library(BrainEnrich)

# Set other parameters
output_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed"
data_path <- "/gpfs1/home/z/c/zcao4/BrainEnrich/data"

# Load hcp brain_data
brain_data <- readRDS(file.path(data_path, "hcp_brain_data_dk_lh.RDS"))
# Load additional necessary data
gene_data <- get_geneExp(atlas = "desikan", rdonor = "r0.4", hem = "L")
annoData <- get_annoData(type = "GO_MF")

if (null_model == "spin_brain") {
  perm_id <- perm_id_dk_lh_5000
} else {
  perm_id <- NULL
}

# Update the output directory based on the input parameters
output_dir <- file.path(
  output_dir_base,
  sprintf(
    "null_%s_%s_%s_%d_%d_%d",
    null_model,
    cor_method,
    aggre_method,
    minGSSize,
    maxGSSize,
    perm_total
  )
)


# Call the brainscore.hpc function
brainscore.hpc(
  job_id = job_id,
  n_perm_per_job = n_perm_per_job,
  perm_total = perm_total,
  perm_id = perm_id,
  output_dir = output_dir,
  brain_data = brain_data,
  gene_data = gene_data,
  annoData = annoData,
  cor_method = cor_method,
  aggre_method = aggre_method,
  null_model = null_model,
  n_cores = 1,
  minGSSize = minGSSize,
  maxGSSize = maxGSSize
)
```

```{bash}
#!/bin/bash
#SBATCH --job-name=gsScore       # Job name
#SBATCH --output=gsScore.out  
#SBATCH --error=gsScore.err   
#SBATCH --time=10:00:00                 # Time limit hrs:min:sec
#SBATCH --nodes=1                       # Number of nodes
#SBATCH --ntasks=1                      # Number of tasks
#SBATCH --mem=16G                       # Memory per node
#SBATCH --array=1-1000                     # Array range (adjust based on perm_total / n_perm_per_job)


# Define variables for the script
n_iter_per_job=5          # Number of permutations per job (adjust as needed)
iter_total=5000           # Total number of permutations
cor_method="pearson"      # Correlation method
aggre_method="mean"       # Aggregation method
minGSSize=20              # Minimum gene set size
maxGSSize=200             # Maximum gene set size

# Define an array of null models to loop through
null_models=("resample_gene" "spin_brain")
gs_types=("SynGO" "GO_MF")

# Change directory to where your R script is located
cd /gpfs1/home/z/c/zcao4/BrainEnrich

for gs_type in "${gs_types[@]}"; do
  for null_model in "${null_models[@]}"; do
    Rscript --vanilla run_brainscore_hpc.R $SLURM_ARRAY_TASK_ID $n_iter_per_job $iter_total $cor_method $aggre_method $null_model $gs_type $minGSSize $maxGSSize
  done
done
```

if there are some job failed, you can do this to rerun the failed jobs
```{bash}
#!/bin/bash
#SBATCH --job-name=gsScore_retry       # Job name
#SBATCH --output=gsScore_retry_%A_%a.out     # Standard output log
#SBATCH --error=gsScore_retry_%A_%a.err      # Standard error log
#SBATCH --time=10:00:00                # Time limit hrs:min:sec
#SBATCH --nodes=1                      # Number of nodes
#SBATCH --ntasks=1                     # Number of tasks
#SBATCH --mem=16G                      # Memory per node
#SBATCH --array=1-5                    # Array range to cover all indices

# Define variables for the script
n_iter_per_job=5          # Number of permutations per job (adjust as needed)
iter_total=5000           # Total number of permutations
cor_method="pearson"      # Correlation method
aggre_method="mean"       # Aggregation method
minGSSize=20              # Minimum gene set size
maxGSSize=200             # Maximum gene set size
null_model="resample_gene" # Null model
gs_type="GO_MF"           # Gene set type

# Define the list of missing job IDs
job_id_list=(70 74 75 76 79)

# Get the specific job ID based on the SLURM_ARRAY_TASK_ID
job_id=${job_id_list[$SLURM_ARRAY_TASK_ID-1]}

# Change directory to where your R script is located
cd /gpfs1/home/z/c/zcao4/BrainEnrich

# Run the R script for the specific jobs
Rscript --vanilla run_brainscore_hpc.R $job_id $n_iter_per_job $iter_total $cor_method $aggre_method $null_model $gs_type $minGSSize $maxGSSize
```

collect the results from jobs
```{r}
library(BrainEnrich)

# Set the base output directory
input_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed_brainscore"

# Define the parameters of the analysis
n_iter_per_job <- 5          # Number of iterations per job (adjust as needed)
iter_total <- 5000           # Total number of iterations
cor_method <- "pearson"      # Correlation method
aggre_method <- "mean"       # Aggregation method
minGSSize <- 20              # Minimum gene set size
maxGSSize <- 200             # Maximum gene set size

# Define an array of null models and gene set types to loop through
null_models <- c("resample_gene", "spin_brain")
gs_types <- c("SynGO", "GO_MF")

# Loop through each combination of null model and gene set type
for (null_model in null_models) {
  for (gs_type in gs_types) {
    # Construct the folder name based on the parameters
    folder_name <- sprintf(
      "%s_%s_%s_%s_%d_%d_%d",
      gs_type, null_model, cor_method, aggre_method, minGSSize, maxGSSize, iter_total
    )
    
    # Construct the full path to the folder
    input_dir <- file.path(input_dir_base, folder_name)
    output_dir <- input_dir_base
    
    # Construct the save name based on the folder name
    save_name <- folder_name
    
    # Call the job_cat function to merge the RDS files in this folder
    job_cat(
      input_dir = input_dir,
      output_dir = output_dir,
      n_rds = iter_total / n_iter_per_job,
      save_name = save_name,
      file_pattern = "res_job_%d.rds",
      delete_originals = TRUE,
      preserve_attributes = TRUE,
      result_prefix = "null_"
    )
  }
}
```



# 7. use job_splitter (a generic function to split jobs) to do brainscore.simulate

```{r}
# job_splitter_brainscore_simulate.R
# Rscript job_splitter_brainscore_simulate.R <job_id> <n_iter_per_job> <iter_total> <cor_method> <aggre_method> <sim_type> <gs_type> <minGSSize> <maxGSSize> <gsScoreList.null>
# Get command-line arguments
args <- commandArgs(trailingOnly = TRUE)
job_id <- as.integer(args[1])
n_iter_per_job <- as.integer(args[2])
iter_total <- as.integer(args[3])
cor_method <- args[4]
aggre_method <- args[5]
sim_type <- args[6]
gs_type <- args[7]
minGSSize <- as.integer(args[8])
maxGSSize <- as.integer(args[9])
gsScoreList.null <- args[10]

# Load your R package and any required libraries
library(BrainEnrich)

# Set other parameters
output_dir_base <- "/gpfs1/home/z/c/zcao4/BrainEnrich/test_job_splitter_sim"
data_path <- "/gpfs1/home/z/c/zcao4/BrainEnrich/data"

# Load hcp brain_data
brain_data <- readRDS(file.path(data_path, "hcp_brain_data_dk_lh.RDS"))
cov_df <- readRDS(file.path(data_path, "hcp_cov_df.RDS"))  # Assuming covariates data is stored in cov_df.RDS
pred_df <- readRDS(file.path(data_path, "hcp_pred_df.RDS"))  # Assuming predictor data is stored in pred_df.RDS

# Load additional necessary data
gene_data <- get_geneExp(atlas = "desikan", rdonor = "r0.4", hem = "L")
annoData <- get_annoData(type = gs_type)

if (sim_type == "spin_brain") {
  subset_vars<-list(perm_id = perm_id_dk_lh_5000)
} else {
  subset_vars<-list()
}

if (sim_type %in% c("resample_gene","spin_brain")){
  gsScoreList.null <- readRDS(gsScoreList.null)
}

# Update the output directory based on the input parameters
output_dir <- file.path(
  output_dir_base,
  sprintf(
    "%s_%s_%s_%s_%d_%d_%d",
    gs_type,
    sim_type,
    cor_method,
    aggre_method,
    minGSSize,
    maxGSSize,
    iter_total
  )
)

# Call the job_splitter function
job_splitter(
  job_id = job_id,
  n_iter_per_job = n_iter_per_job,
  iter_total = iter_total,
  output_dir = output_dir,
  FUN = brainscore.simulate,
  subset_vars = subset_vars,  # No subset_vars required in this case
  subset_total_var = 'sim_n',
  brain_data = brain_data,
  gene_data = gene_data,
  annoData = annoData,
  pred_df = pred_df,
  cov_df = cov_df,
  cor_method = cor_method,
  aggre_method = aggre_method,
  subsample_size = c(50,100,150,200,250,300),
  sim_type = sim_type,
  n_cores = 1,
  minGSSize = minGSSize,
  maxGSSize = maxGSSize,
  gsScoreList.null = gsScoreList.null  # This argument is now correctly placed at the end
)

```

```{bash}
#!/bin/bash
#SBATCH --job-name=brainscore_sim    # Job name
#SBATCH --output=brainscore_sim_%A_%a.out  # Standard output log
#SBATCH --error=brainscore_sim_%A_%a.err   # Standard error log
#SBATCH --time=10:00:00             # Time limit hrs:min:sec
#SBATCH --nodes=1                    # Number of nodes
#SBATCH --ntasks=1                   # Number of tasks
#SBATCH --mem=16G                    # Memory per node
#SBATCH --array=1-5               # Array range (adjust based on iter_total / n_iter_per_job)

# Define the variables
n_iter_per_job=2          # Number of iterations per job
iter_total=10           # Total number of iterations
cor_method="pearson"      # Correlation method
aggre_method="mean"       # Aggregation method
minGSSize=20              # Minimum gene set size
maxGSSize=200             # Maximum gene set size
sim_types=("randomize_pred" "spin_brain" "resample_gene")  # Simulation types
gs_types=("SynGO")  # Gene set types

# Change to the directory where the R script is located
cd /gpfs1/home/z/c/zcao4/BrainEnrich

# Loop through each combination of simulation type and gene set type
for gs_type in "${gs_types[@]}"; do
  for sim_type in "${sim_types[@]}"; do
    
    if [[ "$sim_type" == "randomize_pred" ]]; then
      # Run the script without gsScoreList.null for randomize_pred
      Rscript --vanilla job_splitter_brainscore_simulate.R $SLURM_ARRAY_TASK_ID $n_iter_per_job $iter_total $cor_method $aggre_method $sim_type $gs_type $minGSSize $maxGSSize ""
    else
      # Set the gsScoreList.null file path based on the current analysis parameters
      gsScoreList_null="/gpfs1/home/z/c/zcao4/BrainEnrich/precomputed_brainscore/${gs_type}_${sim_type}_${cor_method}_${aggre_method}_${minGSSize}_${maxGSSize}_${iter_total}.rds"
      
      # Run the script with gsScoreList.null for other simulation types
      Rscript --vanilla job_splitter_brainscore_simulate.R $SLURM_ARRAY_TASK_ID $n_iter_per_job $iter_total $cor_method $aggre_method $sim_type $gs_type $minGSSize $maxGSSize $gsScoreList_null
    fi
    
  done
done

```
